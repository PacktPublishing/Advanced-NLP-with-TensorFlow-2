{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:38.862963Z",
     "start_time": "2020-10-01T21:53:37.344493Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "tf.random.set_seed(42)  # for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:40.265307Z",
     "start_time": "2020-10-01T21:53:38.891217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
    "\n",
    "# chck if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:41.277751Z",
     "start_time": "2020-10-01T21:53:40.514853Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:46.161388Z",
     "start_time": "2020-10-01T21:53:42.619559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "All model checkpoint weights were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "gpttokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "gpt = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:54:50.685322Z",
     "start_time": "2020-10-01T21:54:38.240472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5846 9259  544  481]], shape=(1, 4), dtype=int32)\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "robotics is the only way to get to the surface. \" \n",
      " \" i'm not sure i understand. \" \n",
      " \" the first thing we have to do is find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpttokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "print(input_ids)\n",
    "greedy_output = gpt.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(gpttokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:16.061027Z",
     "start_time": "2020-10-01T21:55:13.333259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "gpt2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", \n",
    "                                         pad_token_id=gpt2tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:23.517246Z",
     "start_time": "2020-10-01T21:55:17.435194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular of the future.\n",
      "\n",
      "The future is not a future where robots are going to be able to do anything. It's a future where robots are going to be able to do anything.\n",
      "\n",
      "The future is\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = gpt2tokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = gpt2.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:35.102811Z",
     "start_time": "2020-10-01T21:55:27.887836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular of science fiction and fantasy. It's a genre that has been around for a long time. It's a genre that has been around for a long time. It's a genre that has been around for a long time\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # for reproducible results\n",
    "# BEAM SEARCH\n",
    "# activate beam search and early_stopping\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=51, \n",
    "    num_beams=20, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:51.708801Z",
     "start_time": "2020-10-01T21:55:41.049036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular term for a new kind of robot. It's a robot that can do a lot of things, but it can't do them all. It can do things that other robots can't.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 3\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:56:18.662519Z",
     "start_time": "2020-10-01T21:56:06.291288Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "\n",
      "0: Robotics is the vernacular of the future.\n",
      "\n",
      "The future of robotics is in the hands of a group of people who have been working on it for a long time. The group is called the Robotics Society of America, or RSA\n",
      "\n",
      "1: Robotics is the vernacular of the future.\n",
      "\n",
      "The future of robotics is in the hands of a group of people who have been working on it for a long time. The group is called the Robotics Institute, and it's led by\n",
      "\n",
      "2: Robotics is the vernacular of the future.\n",
      "\n",
      "The future of robotics is in the hands of a group of people who have been working on it for a long time. The group is called the Robotics Society of America (RSSA).\n"
     ]
    }
   ],
   "source": [
    "# Returning multiple beams\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_outputs = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=7, \n",
    "    no_repeat_ngram_size=3, \n",
    "    num_return_sequences=3,  \n",
    "    early_stopping=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"\\n{}: {}\".format(i, \n",
    "                        gpt2tokenizer.decode(beam_output, \n",
    "                                             skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:56:37.968897Z",
     "start_time": "2020-10-01T21:56:31.887781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the xtrapheatre equivalent. When I started to design drones with some basic knowledge in machine design—what is it, exactly--and the right software, a little bit about human movement and the ability of the person being able\n"
     ]
    }
   ],
   "source": [
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    do_sample=True, \n",
    "    top_k=25,\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:57:44.012864Z",
     "start_time": "2020-10-01T21:57:18.453191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a urn with four thousand five hundred-year fragments. Here were scattered five thousand years in their fragments—what is not, you may not say, four different eras; and the three fragments of the same date, three hundred and seventy-three years, which I am sure of, were all separated into the one hundred and twenty-two pieces to the earth's circumference. It may then be said, therefore, to us that the period of the sixteenth earth-days is the seven hundredth annular year, and we shall learn from that, the twelveteenth is the last earth-year and the eighty-fifth is the last year. It is this day which we shall learn of; it should be, then, therefore, to the fourteenth, the fourteenth being the fourth and the fifty-first of all six hundred-years, the fourth to the twenty-third, the second to the twenty-fourth, the last to\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tokenizer.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:57:53.136325Z",
     "start_time": "2020-10-01T21:57:44.063001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Another sample with a larger model\n",
    "gpt2tok_l = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "gpt2_l = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", \n",
    "                                         pad_token_id=gpt2tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:59:04.169629Z",
     "start_time": "2020-10-01T21:57:53.178268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a ursine creature standing at the edge of a pond. Its face was as white as snow and it looked to be sleeping. It had a red nose, a nose so large that it was like it was made of the face of a dog. The water beneath its feet had a red colour and it smelled of blood.\"\n",
      "\n",
      "The poem was written by Joseph Campbell and later published as The Hero With a Thousand Faces. Campbell's poem is known as the story of the wolf (as is the case for most of his other work). It begins, \"You're walking along a path between the hills. In each direction you see another person or thing of interest.\" The person or thing of interest here being a wolf which had been feeding its young. The only problem with this story is that in the context of a poem about wolves, it's difficult to say what interest the wolf has. The poem does, however, offer a number of clues\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tok_l.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2_l.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tok_l.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf23nlp",
   "language": "python",
   "name": "tf23nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "366px",
    "left": "1112px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
