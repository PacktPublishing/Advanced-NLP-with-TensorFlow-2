{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:40:48.710705Z",
     "start_time": "2020-06-11T05:40:47.120111Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:40:49.366836Z",
     "start_time": "2020-06-11T05:40:48.769944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
    "\n",
    "# chck if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:57:54.542637Z",
     "start_time": "2020-06-10T08:57:54.209270Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T22:10:20.104737Z",
     "start_time": "2020-06-10T22:10:17.354766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "gpttokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "gpt = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T22:10:38.813110Z",
     "start_time": "2020-06-10T22:10:26.712565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 5846  9259   544   481 15611   498]], shape=(1, 6), dtype=int32)\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "robotics is the domain of the government. \" \n",
      " \" i see. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i'm sure you do. \" \n",
      " \" i\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpttokenizer.encode('Robotics is the domain of ', return_tensors='tf')\n",
    "print(input_ids)\n",
    "greedy_output = gpt.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(gpttokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T05:40:55.949520Z",
     "start_time": "2020-06-11T05:40:52.765472Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "gpt2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", \n",
    "                                         pad_token_id=gpt2tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T06:37:52.816190Z",
     "start_time": "2020-06-11T06:37:46.922438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the domain of the United States Government.\n",
      "\n",
      "The United States Government is the primary source of information on the use of drones in the United States.\n",
      "\n",
      "The United States Government is the primary source of information on the use of drones\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = gpt2tokenizer.encode('Robotics is the domain of ', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = gpt2.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:03:51.921003Z",
     "start_time": "2020-06-11T07:03:45.067361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the domain of science, technology, engineering, mathematics, and mathematics (STEM). It is the domain of science, technology, engineering, mathematics, and mathematics (STEM). It is the domain of science, technology, engineering, mathematics,\n"
     ]
    }
   ],
   "source": [
    "# BEAM SEARCH\n",
    "# activate beam search and early_stopping\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=20, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T06:59:22.864593Z",
     "start_time": "2020-06-11T06:59:12.959259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the domain of science and technology.\n",
      "\n",
      "In this article, we will look at some of the most important aspects of robotics and how they can be used to improve the lives of people around the world. We will also look at the\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 3\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T07:16:19.376223Z",
     "start_time": "2020-06-11T07:16:07.737009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "\n",
      "0: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, and the European Union.\n",
      "\n",
      "\n",
      "\n",
      "1: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, France, Germany, Italy, Japan, and the European Union.\n",
      "\n",
      "\n",
      "2: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, the European Union, and the United\n"
     ]
    }
   ],
   "source": [
    "# Returning multiple beams\n",
    "beam_outputs = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=7, \n",
    "    no_repeat_ngram_size=3, \n",
    "    num_return_sequences=3,  \n",
    "    early_stopping=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"\\n{}: {}\".format(i, \n",
    "                        gpt2tokenizer.decode(beam_output, \n",
    "                                             skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T08:38:17.590270Z",
     "start_time": "2020-06-11T08:38:11.737258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the domain of people with multiple careers working with robotics systems. The purpose of Robotics & Machine Learning in Science and engineering research is not necessarily different for any given research type because the results would be much more diverse.\n",
      "\n",
      "\n",
      "Our team uses\n"
     ]
    }
   ],
   "source": [
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    do_sample=True, \n",
    "    top_k=25,\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T08:39:49.085636Z",
     "start_time": "2020-06-11T08:39:24.231617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a sudden appearance of light.\n",
      "\n",
      "Sighing, Xiao Chen slowly stood up and looked at Tian Cheng standing over. He took a step to look closely at Tian Cheng's left wrist and frowned.\n",
      "\n",
      "Lin Feng was startled, and quickly took out a long sword!\n",
      "\n",
      "Lin Feng didn't understand what sort of sword that Long Fei had wielded in the Black and Crystal Palace!\n",
      "\n",
      "The Black and Crystal Palace was completely different than his original Black Stone City. Long Fei carried a sword as a souvenir, which had been placed on the back of his father's arm by Tian Cheng.\n",
      "\n",
      "He drew the sword from his dad's arm again!\n",
      "\n",
      "The black blade was one of the most valuable weapons within the Black and Crystal Palace. The sword was just as sharp as the sharpest of all weapons, which had been placed on Long Fei's father's arm by the Black Stone City's Black Ice, for him to\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tokenizer.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T09:07:46.360264Z",
     "start_time": "2020-06-11T09:06:34.644473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a sudden light and a strange sound. I quickly realized that it was some kind of magic or a dream.\"\n",
      "\n",
      "\"So that's what it is then,\" I said. She shrugged and said, \"I'm not going to say more about it.\" I smiled at her and said, \"Okay!\" but she looked up and said, \"It's just not for me.\"\n",
      "\n",
      "I couldn't keep my smile from showing, so I let out a sigh. She looked very happy about it, which I appreciated, so I said, \"Yeah, but I don't think you should get upset.\"\n",
      "\n",
      "\"I know,\" she said. Then she looked at me and said, \"That's right, I can take it or leave it.\"\n",
      "\n",
      "I didn't know what to say to that, so my face fell more and more. \"I…\" I said. I felt a bit embarrassed with myself. Her eyes narrowed at\n"
     ]
    }
   ],
   "source": [
    "# Another sample with a larger model\n",
    "#gpt2tok_l = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "#gpt2_l = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", \n",
    "#                                         pad_token_id=gpt2tokenizer.eos_token_id)\n",
    "input_ids = gpt2tok_l.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2_l.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tok_l.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF 2.1 (GPU)/Py3.7.5",
   "language": "python",
   "name": "tf21g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "366px",
    "left": "1112px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
