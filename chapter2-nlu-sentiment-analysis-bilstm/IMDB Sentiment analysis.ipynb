{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:13.528704Z",
     "start_time": "2020-10-01T07:03:11.616714Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:30.971832Z",
     "start_time": "2020-10-01T07:03:30.919332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:17.819473Z",
     "start_time": "2020-10-01T07:04:16.967326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "# check if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# enable this line if you wish to see whether GPU/CPU executes\n",
    "# each command\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:34.432270Z",
     "start_time": "2020-10-01T07:04:34.425717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract_reasoning, aeslc, aflw2k3d, ai2_arc, amazon_us_reviews, anli, arc, bair_robot_pushing_small, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, blimp, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, clinc_oos, cmaterdb, cnn_dailymail, coco, coil100, colorectal_histology, colorectal_histology_large, common_voice, cos_e, cosmos_qa, covid19sum, crema_d, curated_breast_imaging_ddsm, cycle_gan, deep_weeds, definite_pronoun_resolution, dementiabank, diabetic_retinopathy_detection, div2k, dmlab, downsampled_imagenet, dsprites, dtd, duke_ultrasound, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, forest_fires, fuss, gap, geirhos_conflict_stimuli, german_credit_numeric, gigaword, glue, groove, higgs, horses_or_humans, i_naturalist2017, imagenet2012, imagenet2012_corrupted, imagenet2012_real, imagenet2012_subset, imagenet_a, imagenet_resized, imagenet_v2, imagenette, imagewang, imdb_reviews, irc_disentanglement, iris, kitti, kmnist, lfw, librispeech, librispeech_lm, libritts, ljspeech, lm1b, lost_and_found, lsun, malaria, math_dataset, mctaco, mnist, mnist_corrupted, movie_lens, movie_rationales, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, newsroom, nsynth, nyu_depth_v2, omniglot, open_images_challenge2019_detection, open_images_v4, openbookqa, opinion_abstracts, opinosis, opus, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, pet_finder, pg19, places365_small, plant_leaves, plant_village, plantae_k, qa4mre, quickdraw_bitmap, reddit, reddit_disentanglement, reddit_tifu, resisc45, robonet, rock_paper_scissors, rock_you, samsum, savee, scan, scene_parse150, scicite, scientific_papers, shapes3d, smallnorb, snli, so2sat, speech_commands, squad, stanford_dogs, stanford_online_products, starcraft_video, stl10, sun397, super_glue, svhn_cropped, ted_hrlr_translate, ted_multi_translate, tedlium, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trivia_qa, uc_merced, ucf101, vctk, vgg_face2, visual_domain_decathlon, voc, voxceleb, voxforge, waymo_open_dataset, web_questions, wider_face, wiki40b, wikihow, wikipedia, wikipedia_toxicity_subtypes, winogrande, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, wordnet, xnli, xsum, yelp_polarity_reviews'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available tfds data sets\n",
    "\", \".join(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:37.653833Z",
     "start_time": "2020-10-01T07:04:37.554098Z"
    }
   },
   "outputs": [],
   "source": [
    "# using TFDS dataset\n",
    "# note: as_supervised converts dicts to tuples\n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n",
    "                                with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n",
    "                      as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:40.136066Z",
     "start_time": "2020-10-01T07:04:40.130384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    version=1.0.0,\n",
      "    description='Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=100000,\n",
      "    splits={\n",
      "        'test': 25000,\n",
      "        'train': 25000,\n",
      "        'unsupervised': 50000,\n",
      "    },\n",
      "    supervised_keys=('text', 'label'),\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:43.390551Z",
     "start_time": "2020-10-01T07:04:43.326285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:54.364677Z",
     "start_time": "2020-10-01T07:04:47.490172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the default tokenizer settings\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:55.711915Z",
     "start_time": "2020-10-01T07:05:55.538523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set, \n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:57.667656Z",
     "start_time": "2020-10-01T07:05:57.624948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:59.963100Z",
     "start_time": "2020-10-01T07:05:59.954134Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformation fucntions to be used with the dataset\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:00.776090Z",
     "start_time": "2020-10-01T07:06:00.703019Z"
    }
   },
   "outputs": [],
   "source": [
    "# test the transformation on a small subset\n",
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:01.845914Z",
     "start_time": "2020-10-01T07:06:01.641109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[11656 31784 37897 22934 71575 81905 19681 30209 57969 20389 12638 22613\n",
      " 21862  5499 22944 58608  2155 71664 83985 23849 80304 23571 48259 81535\n",
      " 49548 57969 37115 78725  6916 12638 43953 58008 37115 23849 92552 69141\n",
      " 45856 53359 48259 81905 56690 43180 51447 11656 81905 50648 37897 84637\n",
      " 80835 45507 14443 27814 35500 29509 36671 68930 91084 91855 73442 20914\n",
      " 71778 42677 91084 45537 37115 78395 13018 27293 88812 38487 32134 49986\n",
      " 33801 31800 24165 57465 89656 38318 87284  5499 31784 28425 23571 21457\n",
      " 36671 45794 69918 12638 21457 81905 31587 31784 36823 14883 49480 21767\n",
      " 32097 90892 89245 52162 31587 90597 83985 23628 46411 48259  8865  8715\n",
      " 56690 46411 21862  5499 56690 23123 18741 90892 69141 18287 49825 39206\n",
      " 46121     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst.take(1):\n",
    "    print(review, label)\n",
    "    print(imdb_encoder.decode(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:03.379137Z",
     "start_time": "2020-10-01T07:06:03.359510Z"
    }
   },
   "outputs": [],
   "source": [
    "#now tokenize/encode/pad all training\n",
    "# and testing data\n",
    "encoded_train = imdb_train.map(encode_tf_fn,\n",
    "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "encoded_test = imdb_test.map(encode_tf_fn,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:19.472832Z",
     "start_time": "2020-10-01T07:06:19.467992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:21.557245Z",
     "start_time": "2020-10-01T07:06:21.550589Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:26.612154Z",
     "start_time": "2020-10-01T07:06:25.825486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (100, None, 64)           6011584   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (100, 64)                 33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 1)                  65        \n",
      "=================================================================\n",
      "Total params: 6,044,673\n",
      "Trainable params: 6,044,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_lstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:31.817335Z",
     "start_time": "2020-10-01T07:06:31.799369Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:37.541747Z",
     "start_time": "2020-10-01T07:06:37.536302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:27.683509Z",
     "start_time": "2020-10-01T07:06:40.292699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.4106 - accuracy: 0.8049 - precision: 0.7849 - recall: 0.8400\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.1692 - accuracy: 0.9380 - precision: 0.9403 - recall: 0.9353\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.1109 - accuracy: 0.9622 - precision: 0.9640 - recall: 0.9603\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 22s 88ms/step - loss: 0.0674 - accuracy: 0.9777 - precision: 0.9781 - recall: 0.9773\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 22s 87ms/step - loss: 0.0552 - accuracy: 0.9823 - precision: 0.9819 - recall: 0.9828\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 23s 90ms/step - loss: 0.0654 - accuracy: 0.9774 - precision: 0.9770 - recall: 0.9779\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 22s 89ms/step - loss: 0.0257 - accuracy: 0.9924 - precision: 0.9921 - recall: 0.9928\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 22s 89ms/step - loss: 0.0117 - accuracy: 0.9973 - precision: 0.9975 - recall: 0.9970\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 23s 90ms/step - loss: 0.0115 - accuracy: 0.9969 - precision: 0.9976 - recall: 0.9962\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 22s 89ms/step - loss: 0.0041 - accuracy: 0.9993 - precision: 0.9994 - recall: 0.9992 0s - loss: 0.0039 - accuracy: 0.9993 - precision: 0.9993 - recall: 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc89c58d410>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:55.965511Z",
     "start_time": "2020-10-01T07:10:34.119008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 21s 82ms/step - loss: 0.9765 - accuracy: 0.8317 - precision: 0.8013 - recall: 0.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9764885902404785,\n",
       " 0.8317199945449829,\n",
       " 0.8013225793838501,\n",
       " 0.8821600079536438]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:23.892847Z",
     "start_time": "2020-10-01T07:11:23.887880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:25.201887Z",
     "start_time": "2020-10-01T07:11:25.192055Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.441086Z",
     "start_time": "2020-10-01T07:11:26.487139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (50, None, 128)           12023168  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (50, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (50, None, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (50, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (50, 128)                 98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (50, 128)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (50, 1)                   129       \n",
      "=================================================================\n",
      "Total params: 12,220,929\n",
      "Trainable params: 12,220,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm = build_model_bilstm(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.575709Z",
     "start_time": "2020-10-01T07:11:29.562501Z"
    }
   },
   "outputs": [],
   "source": [
    "bilstm.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:32.251708Z",
     "start_time": "2020-10-01T07:11:32.245767Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:29.418810Z",
     "start_time": "2020-10-01T07:11:34.295046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.3763 - accuracy: 0.8266 - precision: 0.8205 - recall: 0.8362\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.1505 - accuracy: 0.9465 - precision: 0.9463 - recall: 0.9467\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.0701 - accuracy: 0.9761 - precision: 0.9765 - recall: 0.9757\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 68s 137ms/step - loss: 0.0724 - accuracy: 0.9753 - precision: 0.9747 - recall: 0.9758\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.0416 - accuracy: 0.9859 - precision: 0.9856 - recall: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc85262dd10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.fit(encoded_train_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:50.858013Z",
     "start_time": "2020-10-01T07:17:29.532181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 35ms/step - loss: 0.6753 - accuracy: 0.8384 - precision: 0.8459 - recall: 0.8274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6753188967704773,\n",
       " 0.8383600115776062,\n",
       " 0.8459147810935974,\n",
       " 0.8274400234222412]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf23nlp",
   "language": "python",
   "name": "tf23nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
