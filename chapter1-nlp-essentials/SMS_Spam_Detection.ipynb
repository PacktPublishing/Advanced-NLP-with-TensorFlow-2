{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9eSdb1cn4Z",
        "outputId": "573aafdb-15b6-4b0d-9c26-5eb20530fc86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "#from tf.keras.models import Sequential\n",
        "#from tf.keras.layers import Dense\n",
        "import os\n",
        "import io\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSjikP_c0Ba",
        "outputId": "3dde5b7e-2ba0-457f-fff7-5b60363aad9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 1s 3us/step\n",
            "Archive:  /root/.keras/datasets/smsspamcollection.zip\n",
            "  inflating: data/SMSSpamCollection  \n",
            "  inflating: data/readme             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb8AF3wHaKE3",
        "outputId": "5d6f17dd-77c2-443e-b7a6-f587ac81b776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test data reading\n",
        "# lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "# Pre-Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhpfi9lWC5We",
        "outputId": "a9be6690-6b96-4966-e384-c63b2dece6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "spam_dataset = []\n",
        "count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "\n",
        "print(spam_dataset[0])\n",
        "print(\"Spam: \", count)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')\n",
            "Spam:  747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkoqwjizbBS"
      },
      "source": [
        "# Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsuhM6AI1iYM"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFCghF5YLcmb"
      },
      "source": [
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRQZpvvHhI9K"
      },
      "source": [
        "import re\n",
        "\n",
        "# Normalization functions\n",
        "\n",
        "def message_length(x):\n",
        "  # returns total number of characters\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Z]', '', x) # only works in english\n",
        "  return count\n",
        "\n",
        "def num_punctuation(x):\n",
        "  _, count = re.subn(r'\\W', '', x)\n",
        "  return count\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaPsdhs6mkd_"
      },
      "source": [
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
        "df['Length'] = df['Message'].apply(message_length)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oy5mN8nnrde",
        "outputId": "26ca1f14-dee4-4f6a-aba2-bfee884c171e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.134015</td>\n",
              "      <td>5.621636</td>\n",
              "      <td>18.942591</td>\n",
              "      <td>80.443488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340699</td>\n",
              "      <td>11.683233</td>\n",
              "      <td>14.825994</td>\n",
              "      <td>59.841746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  5574.000000  5574.000000  5574.000000  5574.000000\n",
              "mean      0.134015     5.621636    18.942591    80.443488\n",
              "std       0.340699    11.683233    14.825994    59.841746\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX7Ne4NLoPIE"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=df.drop(train.index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FemsnVTto-c6",
        "outputId": "661c6482-7cc8-4964-e1a0-91d57e542375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  4459.000000  4459.000000  4459.000000  4459.000000\n",
              "mean      0.132765     5.519399    18.886522    80.316439\n",
              "std       0.339359    11.405424    14.602023    59.346407\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    35.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    27.000000   122.000000\n",
              "max       1.000000   129.000000   253.000000   910.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEe1TMcApCYS",
        "outputId": "0b1413fc-1e64-40c3-ccf7-f0b30afc87bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.139013</td>\n",
              "      <td>6.030493</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>80.951570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.346116</td>\n",
              "      <td>12.731059</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>61.807655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>123.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>790.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length\n",
              "count  1115.000000  1115.000000  1115.000000  1115.000000\n",
              "mean      0.139013     6.030493    19.166816    80.951570\n",
              "std       0.346116    12.731059    15.694599    61.807655\n",
              "min       0.000000     0.000000     0.000000     2.000000\n",
              "25%       0.000000     1.000000     8.000000    36.000000\n",
              "50%       0.000000     2.000000    15.000000    61.000000\n",
              "75%       0.000000     4.000000    28.000000   123.000000\n",
              "max       1.000000   127.000000   195.000000   790.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9rSWD4i1X3s"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WU6sxf2qcZd"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Adds a densely-connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units, \n",
        "                                  input_dim=input_dims, \n",
        "                                  activation='relu'))\n",
        "\n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwDyHEq-swwC"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcbnfjYtxvu",
        "outputId": "8753e9bb-6f12-4c1a-c9c4-8b64f31a2f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "x_train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Capitals</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3527</th>\n",
              "      <td>161</td>\n",
              "      <td>48</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>40</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3370</th>\n",
              "      <td>69</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>37</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3280</th>\n",
              "      <td>444</td>\n",
              "      <td>114</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3186</th>\n",
              "      <td>65</td>\n",
              "      <td>14</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3953</th>\n",
              "      <td>81</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>38</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4223</th>\n",
              "      <td>65</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4459 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Length  Punctuation  Capitals\n",
              "3690      25            4         1\n",
              "3527     161           48       107\n",
              "724       40            7         1\n",
              "3370      69           17         3\n",
              "468       37            8         1\n",
              "...      ...          ...       ...\n",
              "3280     444          114        44\n",
              "3186      65           14        50\n",
              "3953      81           23         2\n",
              "2768      38            8         2\n",
              "4223      65           14         2\n",
              "\n",
              "[4459 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1aTS9LuFpF"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otOL712wu4tW",
        "outputId": "d08a3f13-20cd-4d3c-d8f6-336373b5bc66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 3.1590 - accuracy: 0.7179\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.4046 - accuracy: 0.8706\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3565 - accuracy: 0.8818\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.8863\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3113 - accuracy: 0.8832\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2884 - accuracy: 0.8876\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2842 - accuracy: 0.8935\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2796 - accuracy: 0.8912\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2750 - accuracy: 0.8912\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.8906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f38f97857f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svFEbDWzccXV",
        "outputId": "a6f5db9a-cd2d-4119-d095-3a4d70c19c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.8915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26615792512893677, 0.8914798498153687]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Aw3qskjDC-j",
        "outputId": "c7aee3c3-22d9-4ca7-bf0e-98492959f6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "y_train_pred = model.predict_classes(x_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-20-ce9f4b8a8791>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73dfRXaFD3F",
        "outputId": "823a40ef-9f2c-4bac-ff79-5f5c0d9d74b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# confusion matrix\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam), \n",
        "                         y_train_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[3723,  144],\n",
              "       [ 290,  302]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmcZKcJqGiAK",
        "outputId": "631c540b-45f9-4147-fa2e-e8a0dc9b2e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(y_train_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([446], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBeCm5eDduc",
        "outputId": "ca51beef-cdb8-48ca-d9de-961caaeb103d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_test_pred = model.predict_classes(x_test)\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[923,  37],\n",
              "       [ 84,  71]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY-ssfsAOusL"
      },
      "source": [
        "# Tokenization and Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MWyaX6IPk2l",
        "outputId": "15d969a0-8e93-49f5-8ba5-17e4246b6134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sentence = 'Go until jurong point, crazy.. Available only in bugis n great world'\n",
        "sentence.split()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go',\n",
              " 'until',\n",
              " 'jurong',\n",
              " 'point,',\n",
              " 'crazy..',\n",
              " 'Available',\n",
              " 'only',\n",
              " 'in',\n",
              " 'bugis',\n",
              " 'n',\n",
              " 'great',\n",
              " 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvEl3bsRDrk",
        "outputId": "0d8a6db4-382d-4191-b57b-561f6b608541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install stanza  # StanfordNLP has become https://github.com/stanfordnlp/stanza/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.6.0+cu101)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRWnB-mNdH66"
      },
      "source": [
        "import stanza"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6N5vvi8dZS1",
        "outputId": "430b2b03-9ce4-4d54-9b05-67c695b54758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "en = stanza.download('en') "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 8.26MB/s]                    \n",
            "2020-10-14 04:13:38 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/default.zip: 100%|██████████| 428M/428M [06:14<00:00, 1.14MB/s]\n",
            "2020-10-14 04:20:01 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxDrEJBBdfkq",
        "outputId": "4b035242-58c4-4b63-f416-1f26531f6558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "en = stanza.Pipeline(lang='en')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-14 04:20:02 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| pos       | ewt       |\n",
            "| lemma     | ewt       |\n",
            "| depparse  | ewt       |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-10-14 04:20:02 INFO: Use device: gpu\n",
            "2020-10-14 04:20:02 INFO: Loading: tokenize\n",
            "2020-10-14 04:20:12 INFO: Loading: pos\n",
            "2020-10-14 04:20:13 INFO: Loading: lemma\n",
            "2020-10-14 04:20:13 INFO: Loading: depparse\n",
            "2020-10-14 04:20:14 INFO: Loading: sentiment\n",
            "2020-10-14 04:20:15 INFO: Loading: ner\n",
            "2020-10-14 04:20:16 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FlNpKzX2ggg",
        "outputId": "c65a5621-d4e8-4eb2-c65a-d476e572cf45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sentence"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Go until jurong point, crazy.. Available only in bugis n great world'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4UixONeSAd"
      },
      "source": [
        "tokenized = en(sentence)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPq8hvA3he4C",
        "outputId": "7f8f5dbe-fe47-46dc-85f9-f4870dbbe6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenized.sentences)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJarHilWhreq",
        "outputId": "f56b007c-c1b4-4c2e-e232-5a8df231f32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for snt in tokenized.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go\n",
            "until\n",
            "jurong\n",
            "point\n",
            ",\n",
            "crazy\n",
            "..\n",
            "<End of Sentence>\n",
            "Available\n",
            "only\n",
            "in\n",
            "bugis\n",
            "n\n",
            "great\n",
            "world\n",
            "<End of Sentence>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6j_Uu1aKr7"
      },
      "source": [
        "## Dependency Parsing Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRlPCZxdffIi",
        "outputId": "536b27cd-ef26-4514-cf3c-b99ca58e647f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "en2 = stanza.Pipeline(lang='en')\n",
        "pr2 = en2(\"Hari went to school\")\n",
        "for snt in pr2.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-14 04:20:48 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| pos       | ewt       |\n",
            "| lemma     | ewt       |\n",
            "| depparse  | ewt       |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-10-14 04:20:48 INFO: Use device: gpu\n",
            "2020-10-14 04:20:48 INFO: Loading: tokenize\n",
            "2020-10-14 04:20:48 INFO: Loading: pos\n",
            "2020-10-14 04:20:49 INFO: Loading: lemma\n",
            "2020-10-14 04:20:49 INFO: Loading: depparse\n",
            "2020-10-14 04:20:50 INFO: Loading: sentiment\n",
            "2020-10-14 04:20:51 INFO: Loading: ner\n",
            "2020-10-14 04:20:52 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": 1,\n",
            "    \"text\": \"Hari\",\n",
            "    \"lemma\": \"Hari\",\n",
            "    \"upos\": \"PROPN\",\n",
            "    \"xpos\": \"NNP\",\n",
            "    \"feats\": \"Number=Sing\",\n",
            "    \"head\": 2,\n",
            "    \"deprel\": \"nsubj\",\n",
            "    \"misc\": \"start_char=0|end_char=4\",\n",
            "    \"ner\": \"S-PERSON\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 2,\n",
            "    \"text\": \"went\",\n",
            "    \"lemma\": \"go\",\n",
            "    \"upos\": \"VERB\",\n",
            "    \"xpos\": \"VBD\",\n",
            "    \"feats\": \"Mood=Ind|Tense=Past|VerbForm=Fin\",\n",
            "    \"head\": 0,\n",
            "    \"deprel\": \"root\",\n",
            "    \"misc\": \"start_char=5|end_char=9\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 3,\n",
            "    \"text\": \"to\",\n",
            "    \"lemma\": \"to\",\n",
            "    \"upos\": \"ADP\",\n",
            "    \"xpos\": \"IN\",\n",
            "    \"head\": 4,\n",
            "    \"deprel\": \"case\",\n",
            "    \"misc\": \"start_char=10|end_char=12\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"id\": 4,\n",
            "    \"text\": \"school\",\n",
            "    \"lemma\": \"school\",\n",
            "    \"upos\": \"NOUN\",\n",
            "    \"xpos\": \"NN\",\n",
            "    \"feats\": \"Number=Sing\",\n",
            "    \"head\": 2,\n",
            "    \"deprel\": \"obl\",\n",
            "    \"misc\": \"start_char=13|end_char=19\",\n",
            "    \"ner\": \"O\"\n",
            "  }\n",
            "]\n",
            "<End of Sentence>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5NI7lL_aPrR"
      },
      "source": [
        "## Japanese Tokenization Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twQsK3NSRh3e",
        "outputId": "7f4213cc-dc13-461c-d7cc-fbc51b2293ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "jp = stanza.download('ja') "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.2MB/s]                    \n",
            "2020-10-14 04:21:10 INFO: Downloading default packages for language: ja (Japanese)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/ja/default.zip: 100%|██████████| 220M/220M [05:35<00:00, 656kB/s] \n",
            "2020-10-14 04:26:50 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRHQL57iRnH2",
        "outputId": "16edfda1-ff6c-47bc-92e0-f8db4a09b9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "jp = stanza.Pipeline(lang='ja')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-14 04:26:50 INFO: Loading these models for language: ja (Japanese):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | gsd     |\n",
            "| pos       | gsd     |\n",
            "| lemma     | gsd     |\n",
            "| depparse  | gsd     |\n",
            "=======================\n",
            "\n",
            "2020-10-14 04:26:50 INFO: Use device: gpu\n",
            "2020-10-14 04:26:50 INFO: Loading: tokenize\n",
            "2020-10-14 04:26:50 INFO: Loading: pos\n",
            "2020-10-14 04:26:51 INFO: Loading: lemma\n",
            "2020-10-14 04:26:51 INFO: Loading: depparse\n",
            "2020-10-14 04:26:52 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM_AJ4mCS2ja"
      },
      "source": [
        "jp_line = jp(\"選挙管理委員会\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMo9DqvMTzX_",
        "outputId": "a94a4a34-0e63-45df-db3a-23a118bcab97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for snt in jp_line.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "選挙\n",
            "管理\n",
            "委員会\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKfjQeOaWtz"
      },
      "source": [
        "# Adding Word Count Feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBK3MokpYa6n"
      },
      "source": [
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = sum( [ len(sentence.tokens) for sentence in doc.sentences] )\n",
        "  return count\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQf6MQNmbazh"
      },
      "source": [
        "#en = snlp.Pipeline(lang='en', processors='tokenize')\n",
        "df['Words'] = df['Message'].apply(word_counts)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLjJV6gAc7Sh",
        "outputId": "dad8acdc-a1f5-413c-c4c8-48599848ed4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.000000</td>\n",
              "      <td>5574.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.134015</td>\n",
              "      <td>5.621636</td>\n",
              "      <td>18.942591</td>\n",
              "      <td>80.443488</td>\n",
              "      <td>19.03319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.340699</td>\n",
              "      <td>11.683233</td>\n",
              "      <td>14.825994</td>\n",
              "      <td>59.841746</td>\n",
              "      <td>13.96163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>9.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>15.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>28.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>209.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length       Words\n",
              "count  5574.000000  5574.000000  5574.000000  5574.000000  5574.00000\n",
              "mean      0.134015     5.621636    18.942591    80.443488    19.03319\n",
              "std       0.340699    11.683233    14.825994    59.841746    13.96163\n",
              "min       0.000000     0.000000     0.000000     2.000000     1.00000\n",
              "25%       0.000000     1.000000     8.000000    36.000000     9.00000\n",
              "50%       0.000000     2.000000    15.000000    61.000000    15.00000\n",
              "75%       0.000000     4.000000    27.000000   122.000000    28.00000\n",
              "max       1.000000   129.000000   253.000000   910.000000   209.00000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pqZ1a3dOg2"
      },
      "source": [
        "#train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "#test=df.drop(train.index)\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lruns2ddQzT"
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8So4_u34daa5",
        "outputId": "029d54a8-1bbe-4d6b-d178-7c3624bd8ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 10.4237 - accuracy: 0.6217\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.8616\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3754 - accuracy: 0.8888\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3256 - accuracy: 0.8971\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3057 - accuracy: 0.8921\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3082 - accuracy: 0.8939\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2950 - accuracy: 0.8941\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2870 - accuracy: 0.8966\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2922 - accuracy: 0.8973\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2857 - accuracy: 0.8941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3856f2fc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw__r38sdv60",
        "outputId": "23b6d879-7d5e-4290-f807-161651c56f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.264749675989151, 0.8968609571456909]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH43JjrwS9Ta"
      },
      "source": [
        "## Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeJuqpdmKI2",
        "outputId": "85481878-bfdb-48c2-b224-5b14f0c21e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install stopwordsiso"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stopwordsiso\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/03/4c5f24b654bb9459f81aa5c1b60b094b804286b99dca9f2e116c9eb01ac8/stopwordsiso-0.6.1-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-kxi8XBTA1r",
        "outputId": "ece1e1cb-6cf9-4cf2-a4e9-6f53eb6d623d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import stopwordsiso as stopwords\n",
        "\n",
        "stopwords.langs()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'af',\n",
              " 'ar',\n",
              " 'bg',\n",
              " 'bn',\n",
              " 'br',\n",
              " 'ca',\n",
              " 'cs',\n",
              " 'da',\n",
              " 'de',\n",
              " 'el',\n",
              " 'en',\n",
              " 'eo',\n",
              " 'es',\n",
              " 'et',\n",
              " 'eu',\n",
              " 'fa',\n",
              " 'fi',\n",
              " 'fr',\n",
              " 'ga',\n",
              " 'gl',\n",
              " 'gu',\n",
              " 'ha',\n",
              " 'he',\n",
              " 'hi',\n",
              " 'hr',\n",
              " 'hu',\n",
              " 'hy',\n",
              " 'id',\n",
              " 'it',\n",
              " 'ja',\n",
              " 'ko',\n",
              " 'ku',\n",
              " 'la',\n",
              " 'lt',\n",
              " 'lv',\n",
              " 'mr',\n",
              " 'ms',\n",
              " 'nl',\n",
              " 'no',\n",
              " 'pl',\n",
              " 'pt',\n",
              " 'ro',\n",
              " 'ru',\n",
              " 'sk',\n",
              " 'sl',\n",
              " 'so',\n",
              " 'st',\n",
              " 'sv',\n",
              " 'sw',\n",
              " 'th',\n",
              " 'tl',\n",
              " 'tr',\n",
              " 'uk',\n",
              " 'ur',\n",
              " 'vi',\n",
              " 'yo',\n",
              " 'zh',\n",
              " 'zu'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE093gWDT57H",
        "outputId": "983b40db-d22e-4c0f-94d9-715d1921c3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sorted(stopwords.stopwords('en'))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'ll\",\n",
              " \"'tis\",\n",
              " \"'twas\",\n",
              " \"'ve\",\n",
              " '10',\n",
              " '39',\n",
              " 'a',\n",
              " \"a's\",\n",
              " 'able',\n",
              " 'ableabout',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abroad',\n",
              " 'abst',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'added',\n",
              " 'adj',\n",
              " 'adopted',\n",
              " 'ae',\n",
              " 'af',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affects',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'ag',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ago',\n",
              " 'ah',\n",
              " 'ahead',\n",
              " 'ai',\n",
              " \"ain't\",\n",
              " 'aint',\n",
              " 'al',\n",
              " 'all',\n",
              " 'allow',\n",
              " 'allows',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amoungst',\n",
              " 'amount',\n",
              " 'an',\n",
              " 'and',\n",
              " 'announce',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anyhow',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anyways',\n",
              " 'anywhere',\n",
              " 'ao',\n",
              " 'apart',\n",
              " 'apparently',\n",
              " 'appear',\n",
              " 'appreciate',\n",
              " 'appropriate',\n",
              " 'approximately',\n",
              " 'aq',\n",
              " 'ar',\n",
              " 'are',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'arent',\n",
              " 'arise',\n",
              " 'around',\n",
              " 'arpa',\n",
              " 'as',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'associated',\n",
              " 'at',\n",
              " 'au',\n",
              " 'auth',\n",
              " 'available',\n",
              " 'aw',\n",
              " 'away',\n",
              " 'awfully',\n",
              " 'az',\n",
              " 'b',\n",
              " 'ba',\n",
              " 'back',\n",
              " 'backed',\n",
              " 'backing',\n",
              " 'backs',\n",
              " 'backward',\n",
              " 'backwards',\n",
              " 'bb',\n",
              " 'bd',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'began',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'beginnings',\n",
              " 'begins',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'beings',\n",
              " 'believe',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'better',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bf',\n",
              " 'bg',\n",
              " 'bh',\n",
              " 'bi',\n",
              " 'big',\n",
              " 'bill',\n",
              " 'billion',\n",
              " 'biol',\n",
              " 'bj',\n",
              " 'bm',\n",
              " 'bn',\n",
              " 'bo',\n",
              " 'both',\n",
              " 'bottom',\n",
              " 'br',\n",
              " 'brief',\n",
              " 'briefly',\n",
              " 'bs',\n",
              " 'bt',\n",
              " 'but',\n",
              " 'buy',\n",
              " 'bv',\n",
              " 'bw',\n",
              " 'by',\n",
              " 'bz',\n",
              " 'c',\n",
              " \"c'mon\",\n",
              " \"c's\",\n",
              " 'ca',\n",
              " 'call',\n",
              " 'came',\n",
              " 'can',\n",
              " \"can't\",\n",
              " 'cannot',\n",
              " 'cant',\n",
              " 'caption',\n",
              " 'case',\n",
              " 'cases',\n",
              " 'cause',\n",
              " 'causes',\n",
              " 'cc',\n",
              " 'cd',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'cf',\n",
              " 'cg',\n",
              " 'ch',\n",
              " 'changes',\n",
              " 'ci',\n",
              " 'ck',\n",
              " 'cl',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'click',\n",
              " 'cm',\n",
              " 'cmon',\n",
              " 'cn',\n",
              " 'co',\n",
              " 'co.',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'computer',\n",
              " 'con',\n",
              " 'concerning',\n",
              " 'consequently',\n",
              " 'consider',\n",
              " 'considering',\n",
              " 'contain',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'copy',\n",
              " 'corresponding',\n",
              " 'could',\n",
              " \"could've\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'couldnt',\n",
              " 'course',\n",
              " 'cr',\n",
              " 'cry',\n",
              " 'cs',\n",
              " 'cu',\n",
              " 'currently',\n",
              " 'cv',\n",
              " 'cx',\n",
              " 'cy',\n",
              " 'cz',\n",
              " 'd',\n",
              " 'dare',\n",
              " \"daren't\",\n",
              " 'darent',\n",
              " 'date',\n",
              " 'de',\n",
              " 'dear',\n",
              " 'definitely',\n",
              " 'describe',\n",
              " 'described',\n",
              " 'despite',\n",
              " 'detail',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'didnt',\n",
              " 'differ',\n",
              " 'different',\n",
              " 'differently',\n",
              " 'directly',\n",
              " 'dj',\n",
              " 'dk',\n",
              " 'dm',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doesnt',\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'done',\n",
              " 'dont',\n",
              " 'doubtful',\n",
              " 'down',\n",
              " 'downed',\n",
              " 'downing',\n",
              " 'downs',\n",
              " 'downwards',\n",
              " 'due',\n",
              " 'during',\n",
              " 'dz',\n",
              " 'e',\n",
              " 'each',\n",
              " 'early',\n",
              " 'ec',\n",
              " 'ed',\n",
              " 'edu',\n",
              " 'ee',\n",
              " 'effect',\n",
              " 'eg',\n",
              " 'eh',\n",
              " 'eight',\n",
              " 'eighty',\n",
              " 'either',\n",
              " 'eleven',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'empty',\n",
              " 'end',\n",
              " 'ended',\n",
              " 'ending',\n",
              " 'ends',\n",
              " 'enough',\n",
              " 'entirely',\n",
              " 'er',\n",
              " 'es',\n",
              " 'especially',\n",
              " 'et',\n",
              " 'et-al',\n",
              " 'etc',\n",
              " 'even',\n",
              " 'evenly',\n",
              " 'ever',\n",
              " 'evermore',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'ex',\n",
              " 'exactly',\n",
              " 'example',\n",
              " 'except',\n",
              " 'f',\n",
              " 'face',\n",
              " 'faces',\n",
              " 'fact',\n",
              " 'facts',\n",
              " 'fairly',\n",
              " 'far',\n",
              " 'farther',\n",
              " 'felt',\n",
              " 'few',\n",
              " 'fewer',\n",
              " 'ff',\n",
              " 'fi',\n",
              " 'fifteen',\n",
              " 'fifth',\n",
              " 'fifty',\n",
              " 'fify',\n",
              " 'fill',\n",
              " 'find',\n",
              " 'finds',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'fj',\n",
              " 'fk',\n",
              " 'fm',\n",
              " 'fo',\n",
              " 'followed',\n",
              " 'following',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'forever',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forth',\n",
              " 'forty',\n",
              " 'forward',\n",
              " 'found',\n",
              " 'four',\n",
              " 'fr',\n",
              " 'free',\n",
              " 'from',\n",
              " 'front',\n",
              " 'full',\n",
              " 'fully',\n",
              " 'further',\n",
              " 'furthered',\n",
              " 'furthering',\n",
              " 'furthermore',\n",
              " 'furthers',\n",
              " 'fx',\n",
              " 'g',\n",
              " 'ga',\n",
              " 'gave',\n",
              " 'gb',\n",
              " 'gd',\n",
              " 'ge',\n",
              " 'general',\n",
              " 'generally',\n",
              " 'get',\n",
              " 'gets',\n",
              " 'getting',\n",
              " 'gf',\n",
              " 'gg',\n",
              " 'gh',\n",
              " 'gi',\n",
              " 'give',\n",
              " 'given',\n",
              " 'gives',\n",
              " 'giving',\n",
              " 'gl',\n",
              " 'gm',\n",
              " 'gmt',\n",
              " 'gn',\n",
              " 'go',\n",
              " 'goes',\n",
              " 'going',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'goods',\n",
              " 'got',\n",
              " 'gotten',\n",
              " 'gov',\n",
              " 'gp',\n",
              " 'gq',\n",
              " 'gr',\n",
              " 'great',\n",
              " 'greater',\n",
              " 'greatest',\n",
              " 'greetings',\n",
              " 'group',\n",
              " 'grouped',\n",
              " 'grouping',\n",
              " 'groups',\n",
              " 'gs',\n",
              " 'gt',\n",
              " 'gu',\n",
              " 'gw',\n",
              " 'gy',\n",
              " 'h',\n",
              " 'had',\n",
              " \"hadn't\",\n",
              " 'hadnt',\n",
              " 'half',\n",
              " 'happens',\n",
              " 'hardly',\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'hasnt',\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'havent',\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'hed',\n",
              " 'hell',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " \"here's\",\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'heres',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'herse”',\n",
              " 'hes',\n",
              " 'hi',\n",
              " 'hid',\n",
              " 'high',\n",
              " 'higher',\n",
              " 'highest',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'himse”',\n",
              " 'his',\n",
              " 'hither',\n",
              " 'hk',\n",
              " 'hm',\n",
              " 'hn',\n",
              " 'home',\n",
              " 'homepage',\n",
              " 'hopefully',\n",
              " 'how',\n",
              " \"how'd\",\n",
              " \"how'll\",\n",
              " \"how's\",\n",
              " 'howbeit',\n",
              " 'however',\n",
              " 'hr',\n",
              " 'ht',\n",
              " 'htm',\n",
              " 'html',\n",
              " 'http',\n",
              " 'hu',\n",
              " 'hundred',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'i.e.',\n",
              " 'id',\n",
              " 'ie',\n",
              " 'if',\n",
              " 'ignored',\n",
              " 'ii',\n",
              " 'il',\n",
              " 'ill',\n",
              " 'im',\n",
              " 'immediate',\n",
              " 'immediately',\n",
              " 'importance',\n",
              " 'important',\n",
              " 'in',\n",
              " 'inasmuch',\n",
              " 'inc',\n",
              " 'inc.',\n",
              " 'indeed',\n",
              " 'index',\n",
              " 'indicate',\n",
              " 'indicated',\n",
              " 'indicates',\n",
              " 'information',\n",
              " 'inner',\n",
              " 'inside',\n",
              " 'insofar',\n",
              " 'instead',\n",
              " 'int',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'interests',\n",
              " 'into',\n",
              " 'invention',\n",
              " 'inward',\n",
              " 'io',\n",
              " 'iq',\n",
              " 'ir',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'isnt',\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'itd',\n",
              " 'itll',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'itse”',\n",
              " 'ive',\n",
              " 'j',\n",
              " 'je',\n",
              " 'jm',\n",
              " 'jo',\n",
              " 'join',\n",
              " 'jp',\n",
              " 'just',\n",
              " 'k',\n",
              " 'ke',\n",
              " 'keep',\n",
              " 'keeps',\n",
              " 'kept',\n",
              " 'keys',\n",
              " 'kg',\n",
              " 'kh',\n",
              " 'ki',\n",
              " 'kind',\n",
              " 'km',\n",
              " 'kn',\n",
              " 'knew',\n",
              " 'know',\n",
              " 'known',\n",
              " 'knows',\n",
              " 'kp',\n",
              " 'kr',\n",
              " 'kw',\n",
              " 'ky',\n",
              " 'kz',\n",
              " 'l',\n",
              " 'la',\n",
              " 'large',\n",
              " 'largely',\n",
              " 'last',\n",
              " 'lately',\n",
              " 'later',\n",
              " 'latest',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'lb',\n",
              " 'lc',\n",
              " 'least',\n",
              " 'length',\n",
              " 'less',\n",
              " 'lest',\n",
              " 'let',\n",
              " \"let's\",\n",
              " 'lets',\n",
              " 'li',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'likely',\n",
              " 'likewise',\n",
              " 'line',\n",
              " 'little',\n",
              " 'lk',\n",
              " 'll',\n",
              " 'long',\n",
              " 'longer',\n",
              " 'longest',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'looks',\n",
              " 'low',\n",
              " 'lower',\n",
              " 'lr',\n",
              " 'ls',\n",
              " 'lt',\n",
              " 'ltd',\n",
              " 'lu',\n",
              " 'lv',\n",
              " 'ly',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'made',\n",
              " 'mainly',\n",
              " 'make',\n",
              " 'makes',\n",
              " 'making',\n",
              " 'man',\n",
              " 'many',\n",
              " 'may',\n",
              " 'maybe',\n",
              " \"mayn't\",\n",
              " 'maynt',\n",
              " 'mc',\n",
              " 'md',\n",
              " 'me',\n",
              " 'mean',\n",
              " 'means',\n",
              " 'meantime',\n",
              " 'meanwhile',\n",
              " 'member',\n",
              " 'members',\n",
              " 'men',\n",
              " 'merely',\n",
              " 'mg',\n",
              " 'mh',\n",
              " 'microsoft',\n",
              " 'might',\n",
              " \"might've\",\n",
              " \"mightn't\",\n",
              " 'mightnt',\n",
              " 'mil',\n",
              " 'mill',\n",
              " 'million',\n",
              " 'mine',\n",
              " 'minus',\n",
              " 'miss',\n",
              " 'mk',\n",
              " 'ml',\n",
              " 'mm',\n",
              " 'mn',\n",
              " 'mo',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'move',\n",
              " 'mp',\n",
              " 'mq',\n",
              " 'mr',\n",
              " 'mrs',\n",
              " 'ms',\n",
              " 'msie',\n",
              " 'mt',\n",
              " 'mu',\n",
              " 'much',\n",
              " 'mug',\n",
              " 'must',\n",
              " \"must've\",\n",
              " \"mustn't\",\n",
              " 'mustnt',\n",
              " 'mv',\n",
              " 'mw',\n",
              " 'mx',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'myse”',\n",
              " 'mz',\n",
              " 'n',\n",
              " 'na',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'nay',\n",
              " 'nc',\n",
              " 'nd',\n",
              " 'ne',\n",
              " 'near',\n",
              " 'nearly',\n",
              " 'necessarily',\n",
              " 'necessary',\n",
              " 'need',\n",
              " 'needed',\n",
              " 'needing',\n",
              " \"needn't\",\n",
              " 'neednt',\n",
              " 'needs',\n",
              " 'neither',\n",
              " 'net',\n",
              " 'netscape',\n",
              " 'never',\n",
              " 'neverf',\n",
              " 'neverless',\n",
              " 'nevertheless',\n",
              " 'new',\n",
              " 'newer',\n",
              " 'newest',\n",
              " 'next',\n",
              " 'nf',\n",
              " 'ng',\n",
              " 'ni',\n",
              " 'nine',\n",
              " 'ninety',\n",
              " 'nl',\n",
              " 'no',\n",
              " 'no-one',\n",
              " 'nobody',\n",
              " 'non',\n",
              " 'none',\n",
              " 'nonetheless',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'normally',\n",
              " 'nos',\n",
              " 'not',\n",
              " 'noted',\n",
              " 'nothing',\n",
              " 'notwithstanding',\n",
              " 'novel',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'np',\n",
              " 'nr',\n",
              " 'nu',\n",
              " 'null',\n",
              " 'number',\n",
              " 'numbers',\n",
              " 'nz',\n",
              " 'o',\n",
              " 'obtain',\n",
              " 'obtained',\n",
              " 'obviously',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'oh',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'old',\n",
              " 'older',\n",
              " 'oldest',\n",
              " 'om',\n",
              " 'omitted',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " \"one's\",\n",
              " 'ones',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'open',\n",
              " 'opened',\n",
              " 'opening',\n",
              " 'opens',\n",
              " 'opposite',\n",
              " 'or',\n",
              " 'ord',\n",
              " 'order',\n",
              " 'ordered',\n",
              " 'ordering',\n",
              " 'orders',\n",
              " 'org',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'ought',\n",
              " \"oughtn't\",\n",
              " 'oughtnt',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'overall',\n",
              " 'owing',\n",
              " 'own',\n",
              " 'p',\n",
              " 'pa',\n",
              " 'page',\n",
              " 'pages',\n",
              " 'part',\n",
              " 'parted',\n",
              " 'particular',\n",
              " 'particularly',\n",
              " 'parting',\n",
              " 'parts',\n",
              " 'past',\n",
              " 'pe',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'pf',\n",
              " 'pg',\n",
              " 'ph',\n",
              " 'pk',\n",
              " 'pl',\n",
              " 'place',\n",
              " 'placed',\n",
              " 'places',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'pmid',\n",
              " 'pn',\n",
              " 'point',\n",
              " 'pointed',\n",
              " 'pointing',\n",
              " 'points',\n",
              " 'poorly',\n",
              " 'possible',\n",
              " 'possibly',\n",
              " 'potentially',\n",
              " 'pp',\n",
              " 'pr',\n",
              " 'predominantly',\n",
              " 'present',\n",
              " 'presented',\n",
              " 'presenting',\n",
              " 'presents',\n",
              " 'presumably',\n",
              " 'previously',\n",
              " 'primarily',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'problems',\n",
              " 'promptly',\n",
              " 'proud',\n",
              " 'provided',\n",
              " 'provides',\n",
              " 'pt',\n",
              " 'put',\n",
              " 'puts',\n",
              " 'pw',\n",
              " 'py',\n",
              " 'q',\n",
              " 'qa',\n",
              " 'que',\n",
              " 'quickly',\n",
              " 'quite',\n",
              " 'qv',\n",
              " 'r',\n",
              " 'ran',\n",
              " 'rather',\n",
              " 'rd',\n",
              " 're',\n",
              " 'readily',\n",
              " 'really',\n",
              " 'reasonably',\n",
              " 'recent',\n",
              " 'recently',\n",
              " 'ref',\n",
              " 'refs',\n",
              " 'regarding',\n",
              " 'regardless',\n",
              " 'regards',\n",
              " 'related',\n",
              " 'relatively',\n",
              " 'research',\n",
              " 'reserved',\n",
              " 'respectively',\n",
              " 'resulted',\n",
              " 'resulting',\n",
              " 'results',\n",
              " 'right',\n",
              " 'ring',\n",
              " 'ro',\n",
              " 'room',\n",
              " 'rooms',\n",
              " 'round',\n",
              " 'ru',\n",
              " 'run',\n",
              " 'rw',\n",
              " 's',\n",
              " 'sa',\n",
              " 'said',\n",
              " 'same',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'sb',\n",
              " 'sc',\n",
              " 'sd',\n",
              " 'se',\n",
              " 'sec',\n",
              " 'second',\n",
              " 'secondly',\n",
              " 'seconds',\n",
              " 'section',\n",
              " 'see',\n",
              " 'seeing',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'sees',\n",
              " 'self',\n",
              " 'selves',\n",
              " 'sensible',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'seven',\n",
              " 'seventy',\n",
              " 'several',\n",
              " 'sg',\n",
              " 'sh',\n",
              " 'shall',\n",
              " \"shan't\",\n",
              " 'shant',\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'shed',\n",
              " 'shell',\n",
              " 'shes',\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'shouldnt',\n",
              " 'show',\n",
              " 'showed',\n",
              " 'showing',\n",
              " 'shown',\n",
              " 'showns',\n",
              " 'shows',\n",
              " 'si',\n",
              " 'side',\n",
              " 'sides',\n",
              " 'significant',\n",
              " 'significantly',\n",
              " 'similar',\n",
              " 'similarly',\n",
              " 'since',\n",
              " 'sincere',\n",
              " 'site',\n",
              " 'six',\n",
              " 'sixty',\n",
              " 'sj',\n",
              " 'sk',\n",
              " 'sl',\n",
              " 'slightly',\n",
              " 'sm',\n",
              " 'small',\n",
              " 'smaller',\n",
              " 'smallest',\n",
              " 'sn',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'someday',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'somethan',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhat',\n",
              " 'somewhere',\n",
              " 'soon',\n",
              " 'sorry',\n",
              " 'specifically',\n",
              " 'specified',\n",
              " 'specify',\n",
              " 'specifying',\n",
              " 'sr',\n",
              " 'st',\n",
              " 'state',\n",
              " 'states',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'strongly',\n",
              " 'su',\n",
              " 'sub',\n",
              " 'substantially',\n",
              " 'successfully',\n",
              " 'such',\n",
              " 'sufficiently',\n",
              " 'suggest',\n",
              " 'sup',\n",
              " 'sure',\n",
              " 'sv',\n",
              " 'sy',\n",
              " 'system',\n",
              " 'sz',\n",
              " 't',\n",
              " \"t's\",\n",
              " 'take',\n",
              " 'taken',\n",
              " 'taking',\n",
              " 'tc',\n",
              " 'td',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUXOkhgnRol"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trbxkc1B4sNP"
      },
      "source": [
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXxl8S1vCGi3",
        "outputId": "55b7ebc1-461c-4134-c4ca-d5edb7aabc7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model = make_model(input_dims=4)\n",
        "#model = make_model(input_dims=3)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.5050 - accuracy: 0.8778\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2953 - accuracy: 0.9159\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2448 - accuracy: 0.9253\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2150 - accuracy: 0.9312\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2045 - accuracy: 0.9316\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1978 - accuracy: 0.9377\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1936 - accuracy: 0.9379\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1891 - accuracy: 0.9363\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.1902 - accuracy: 0.9356\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.2000 - accuracy: 0.9336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3832aa7d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4-0huTw5YFe"
      },
      "source": [
        "## POS Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSXLgr6iiB0",
        "outputId": "5dd25cbe-3fea-4ba9-f3aa-0598a4558e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "en = stanza.Pipeline(lang='en')\n",
        "\n",
        "txt = \"Yo you around? A friend of mine's lookin.\"\n",
        "pos = en(txt)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-14 04:51:48 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | ewt       |\n",
            "| pos       | ewt       |\n",
            "| lemma     | ewt       |\n",
            "| depparse  | ewt       |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2020-10-14 04:51:48 INFO: Use device: gpu\n",
            "2020-10-14 04:51:48 INFO: Loading: tokenize\n",
            "2020-10-14 04:51:48 INFO: Loading: pos\n",
            "2020-10-14 04:51:49 INFO: Loading: lemma\n",
            "2020-10-14 04:51:49 INFO: Loading: depparse\n",
            "2020-10-14 04:51:50 INFO: Loading: sentiment\n",
            "2020-10-14 04:51:51 INFO: Loading: ner\n",
            "2020-10-14 04:51:51 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD9BUu6gkNi_"
      },
      "source": [
        "def print_pos(doc):\n",
        "    text = \"\"\n",
        "    for sentence in doc.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            text += token.words[0].text + \"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        text += \"\\n\"\n",
        "    return text"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTLrN-BgkuWV",
        "outputId": "1799c33f-1820-448e-b074-b3f4732119a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(print_pos(pos))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yo/PRON you/PRON around/ADV ?/PUNCT \n",
            "A/DET friend/NOUN of/ADP mine/PRON 's/PART lookin/NOUN ./PUNCT \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i_BcxgK5cs6"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw and \\\n",
        "        token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "          count += 1\n",
        "  return count"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ddhhiY9FgG",
        "outputId": "acace7c9-9dc7-48bd-df18-d057340c170c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(word_counts(txt), word_counts_v3(txt))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99upx22tCHgz",
        "outputId": "f523ebfe-d63c-4a20-e0d8-75e8f41cc5b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train['Test'] = 0\n",
        "train.describe()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "      <td>9.326979</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "      <td>8.016488</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>147.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  Punctuation       Length        Words    Test\n",
              "count  4459.000000  4459.000000  4459.000000  4459.000000  4459.000000  4459.0\n",
              "mean      0.132765     5.519399    18.886522    80.316439     9.326979     0.0\n",
              "std       0.339359    11.405424    14.602023    59.346407     8.016488     0.0\n",
              "min       0.000000     0.000000     0.000000     2.000000     0.000000     0.0\n",
              "25%       0.000000     1.000000     8.000000    35.000000     4.000000     0.0\n",
              "50%       0.000000     2.000000    15.000000    61.000000     7.000000     0.0\n",
              "75%       0.000000     4.000000    27.000000   122.000000    13.000000     0.0\n",
              "max       1.000000   129.000000   253.000000   910.000000   147.000000     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkdUgEdEDcB"
      },
      "source": [
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  for sentence in doc.sentences:\n",
        "    totals += len(sentence.tokens)  # (1)\n",
        "    for token in sentence.tokens:\n",
        "        if token.text.lower() not in en_sw:\n",
        "          if token.words[0].upos not in ['PUNCT', 'SYM']:\n",
        "            count += 1.\n",
        "          else:\n",
        "            non_word += 1.\n",
        "  non_word = non_word / totals\n",
        "  return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct'])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6LWE7QWIuyu",
        "outputId": "aa347b17-f227-461f-a30f-04b588027483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "x = train[:10]\n",
        "x.describe()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation     Length      Words  Test\n",
              "count  10.0   10.000000    10.000000   10.00000  10.000000  10.0\n",
              "mean    0.0   14.400000    18.300000   72.70000   8.600000   0.0\n",
              "std     0.0   32.948445    14.772723   50.36103  10.068653   0.0\n",
              "min     0.0    1.000000     4.000000   23.00000   2.000000   0.0\n",
              "25%     0.0    1.000000     7.250000   37.75000   3.000000   0.0\n",
              "50%     0.0    1.500000    13.000000   57.00000   4.000000   0.0\n",
              "75%     0.0    9.000000    23.750000   88.00000  10.750000   0.0\n",
              "max     0.0  107.000000    48.000000  161.00000  35.000000   0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Y6_7E8KJnQ",
        "outputId": "57f1ff3d-3fd1-408b-85c8-094570a75804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train_tmp = train['Message'].apply(word_counts_v3)\n",
        "train = pd.concat([train, train_tmp], axis=1)\n",
        "train.describe()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.0</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "      <td>5.519399</td>\n",
              "      <td>18.886522</td>\n",
              "      <td>80.316439</td>\n",
              "      <td>9.326979</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.535995</td>\n",
              "      <td>0.147763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "      <td>11.405424</td>\n",
              "      <td>14.602023</td>\n",
              "      <td>59.346407</td>\n",
              "      <td>8.016488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.679984</td>\n",
              "      <td>0.094337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>910.000000</td>\n",
              "      <td>147.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  ...  Words_NoPunct        Punct\n",
              "count  4459.000000  4459.000000  ...    4459.000000  4459.000000\n",
              "mean      0.132765     5.519399  ...       6.535995     0.147763\n",
              "std       0.339359    11.405424  ...       5.679984     0.094337\n",
              "min       0.000000     0.000000  ...       0.000000     0.000000\n",
              "25%       0.000000     1.000000  ...       3.000000     0.090909\n",
              "50%       0.000000     2.000000  ...       5.000000     0.142857\n",
              "75%       0.000000     4.000000  ...       9.000000     0.200000\n",
              "max       1.000000   129.000000  ...      54.000000     0.666667\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WUdTWXvWbvc",
        "outputId": "70483b7a-ba7e-4530-b5e8-a40142293d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "test_tmp = test['Message'].apply(word_counts_v3)\n",
        "test = pd.concat([test, test_tmp], axis=1)\n",
        "test.describe()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "      <td>1115.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.139013</td>\n",
              "      <td>6.030493</td>\n",
              "      <td>19.166816</td>\n",
              "      <td>80.951570</td>\n",
              "      <td>9.623318</td>\n",
              "      <td>6.700448</td>\n",
              "      <td>0.152936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.346116</td>\n",
              "      <td>12.731059</td>\n",
              "      <td>15.694599</td>\n",
              "      <td>61.807655</td>\n",
              "      <td>8.303803</td>\n",
              "      <td>5.887786</td>\n",
              "      <td>0.101909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>123.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>790.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam     Capitals  ...  Words_NoPunct        Punct\n",
              "count  1115.000000  1115.000000  ...    1115.000000  1115.000000\n",
              "mean      0.139013     6.030493  ...       6.700448     0.152936\n",
              "std       0.346116    12.731059  ...       5.887786     0.101909\n",
              "min       0.000000     0.000000  ...       0.000000     0.000000\n",
              "25%       0.000000     1.000000  ...       3.000000     0.096774\n",
              "50%       0.000000     2.000000  ...       4.000000     0.142857\n",
              "75%       0.000000     4.000000  ...      10.000000     0.200000\n",
              "max       1.000000   127.000000  ...      45.000000     1.000000\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMv1q8_mKdjI",
        "outputId": "c4d4bff6-d9e3-435c-b0d4-73573c3b6791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "z = pd.concat([x, train_tmp], axis=1)\n",
        "z.describe()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>4459.000000</td>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.535995</td>\n",
              "      <td>0.147763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.679984</td>\n",
              "      <td>0.094337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation  ...  Test  Words_NoPunct        Punct\n",
              "count  10.0   10.000000    10.000000  ...  10.0    4459.000000  4459.000000\n",
              "mean    0.0   14.400000    18.300000  ...   0.0       6.535995     0.147763\n",
              "std     0.0   32.948445    14.772723  ...   0.0       5.679984     0.094337\n",
              "min     0.0    1.000000     4.000000  ...   0.0       0.000000     0.000000\n",
              "25%     0.0    1.000000     7.250000  ...   0.0       3.000000     0.090909\n",
              "50%     0.0    1.500000    13.000000  ...   0.0       5.000000     0.142857\n",
              "75%     0.0    9.000000    23.750000  ...   0.0       9.000000     0.200000\n",
              "max     0.0  107.000000    48.000000  ...   0.0      54.000000     0.666667\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkFZUJcPHA3",
        "outputId": "1aba40b1-768f-42e7-cb0f-b6d803503023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "z.loc[z['Spam']==0].describe()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>72.70000</td>\n",
              "      <td>8.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>0.151479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.948445</td>\n",
              "      <td>14.772723</td>\n",
              "      <td>50.36103</td>\n",
              "      <td>10.068653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.412452</td>\n",
              "      <td>0.063396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>37.75000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.130721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>57.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.750000</td>\n",
              "      <td>88.00000</td>\n",
              "      <td>10.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.750000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>161.00000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam    Capitals  Punctuation  ...  Test  Words_NoPunct      Punct\n",
              "count  10.0   10.000000    10.000000  ...  10.0      10.000000  10.000000\n",
              "mean    0.0   14.400000    18.300000  ...   0.0       5.500000   0.151479\n",
              "std     0.0   32.948445    14.772723  ...   0.0       7.412452   0.063396\n",
              "min     0.0    1.000000     4.000000  ...   0.0       1.000000   0.000000\n",
              "25%     0.0    1.000000     7.250000  ...   0.0       2.000000   0.130721\n",
              "50%     0.0    1.500000    13.000000  ...   0.0       2.000000   0.166667\n",
              "75%     0.0    9.000000    23.750000  ...   0.0       6.750000   0.200000\n",
              "max     0.0  107.000000    48.000000  ...   0.0      25.000000   0.208333\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l86GIgYvP9Mc",
        "outputId": "42f8de52-315c-4a29-ac78-c4d7250fbb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "z.loc[z['Spam']==1].describe()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Spam  Capitals  Punctuation  Length  Words  Test  Words_NoPunct  Punct\n",
              "count   0.0       0.0          0.0     0.0    0.0   0.0            0.0    0.0\n",
              "mean    NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "std     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "min     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "25%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "50%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "75%     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN\n",
              "max     NaN       NaN          NaN     NaN    NaN   NaN            NaN    NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PdmMu_R01u"
      },
      "source": [
        "aa = [word_counts_v3(y) for y in x['Message']]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxbhHD_SMPH",
        "outputId": "056a07c1-7490-4e5b-e6af-b4bec050b1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "ab = pd.DataFrame(aa)\n",
        "ab.describe()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.500000</td>\n",
              "      <td>0.151479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.412452</td>\n",
              "      <td>0.063396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.130721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.750000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>25.000000</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Words_NoPunct      Punct\n",
              "count      10.000000  10.000000\n",
              "mean        5.500000   0.151479\n",
              "std         7.412452   0.063396\n",
              "min         1.000000   0.000000\n",
              "25%         2.000000   0.130721\n",
              "50%         2.000000   0.166667\n",
              "75%         6.750000   0.200000\n",
              "max        25.000000   0.208333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__TVLnoy-nre"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbXe_2yL-qb4"
      },
      "source": [
        "\n",
        "text = \"Stemming is aimed at reducing vocabulary and aid un-derstanding of\" +\\\n",
        "       \" morphological processes. This helps people un-derstand the\" +\\\n",
        "       \" morphology of words and reduce size of corpus.\"\n",
        "\n",
        "lemma = en(text)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8VfJ07pDEgN",
        "outputId": "9a5b8b26-de82-416d-ea41-6f7bcec884b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "lemmas = \"\"\n",
        "for sentence in lemma.sentences:\n",
        "        for token in sentence.tokens:\n",
        "            lemmas += token.words[0].lemma +\"/\" + \\\n",
        "                    token.words[0].upos + \" \"\n",
        "        lemmas += \"\\n\"\n",
        "\n",
        "print(lemmas)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stemming/NOUN be/AUX aim/VERB at/SCONJ reduce/VERB vocabulary/NOUN and/CCONJ aid/NOUN un/NOUN -/PUNCT derstanding/NOUN of/ADP morphological/ADJ process/NOUN ./PUNCT \n",
            "this/PRON help/VERB people/NOUN un/NOUN -/PUNCT derstand/VERB the/DET morphology/NOUN of/ADP word/NOUN and/CCONJ reduce/VERB size/NOUN of/ADP corpus/NOUN ./PUNCT \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "# TF-IDF Based Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy7RPjvfDRXz",
        "outputId": "5d0b1786-c4a9-4981-f69a-e6a11113a008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# if not installed already\n",
        "!pip install sklearn"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "          \"I like fruits. Fruits like bananas\",\n",
        "          \"I love bananas but eat an apple\",\n",
        "          \"An apple a day keeps the doctor away\"\n",
        "]\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbftF_xOEN9O"
      },
      "source": [
        "## Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVOFDb1ERTL",
        "outputId": "1325f9bf-f9de-4ec0-c1a9-4507c899b88e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an',\n",
              " 'apple',\n",
              " 'away',\n",
              " 'bananas',\n",
              " 'but',\n",
              " 'day',\n",
              " 'doctor',\n",
              " 'eat',\n",
              " 'fruits',\n",
              " 'keeps',\n",
              " 'like',\n",
              " 'love',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM7jlBZdFmiD",
        "outputId": "b1746158-25ba-4585-c995-9fd35d4c758c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n",
              "       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pbT9sH-GwvM",
        "outputId": "56e3008d-175d-4d69-9839-ff204c7a1cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.13608276, 0.        ],\n",
              "       [0.13608276, 1.        , 0.3086067 ],\n",
              "       [0.        , 0.3086067 , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6qPCQ4Gxb0",
        "outputId": "fea17707-d1b4-4a02-a986-f4ed045f4125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23570226],\n",
              "       [0.57735027],\n",
              "       [0.26726124]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyNZgATHmhM",
        "outputId": "f7f0e74c-81fc-48e2-ef87-8046eab50f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), \n",
        "             columns=vectorizer.get_feature_names())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>an</th>\n",
              "      <th>apple</th>\n",
              "      <th>away</th>\n",
              "      <th>bananas</th>\n",
              "      <th>but</th>\n",
              "      <th>day</th>\n",
              "      <th>doctor</th>\n",
              "      <th>eat</th>\n",
              "      <th>fruits</th>\n",
              "      <th>keeps</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230408</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         an     apple      away  ...      like      love       the\n",
              "0  0.000000  0.000000  0.000000  ...  0.688081  0.000000  0.000000\n",
              "1  0.321267  0.321267  0.000000  ...  0.000000  0.479709  0.000000\n",
              "2  0.275785  0.275785  0.411797  ...  0.000000  0.000000  0.411797\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnG2OPHwD7U",
        "outputId": "39c46fa8-dc12-4391-8f39-7c9d9cc2a37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4459, 7741)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGujuysLwKJe",
        "outputId": "fe7fae62-64b9-480d-e596-02588b22ce61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8874\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 2s 4ms/step - loss: 0.1043 - accuracy: 0.9751\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9899\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0276 - accuracy: 0.9939\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9966\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 0.9987\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0079 - accuracy: 0.9991\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9991\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 2s 3ms/step - loss: 0.0039 - accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0029 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f37d1fb7828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYe6f8my21Y",
        "outputId": "d05d2f28-b07d-4292-b977-2e2e03985195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05765564367175102, 0.9838564991950989]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptf40YDjh71c",
        "outputId": "1139c8e5-4e91-42a7-915d-5303b607db77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Capitals</th>\n",
              "      <th>Punctuation</th>\n",
              "      <th>Length</th>\n",
              "      <th>Words</th>\n",
              "      <th>Test</th>\n",
              "      <th>Words_NoPunct</th>\n",
              "      <th>Punct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>592.0</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.0</td>\n",
              "      <td>592.000000</td>\n",
              "      <td>592.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>15.320946</td>\n",
              "      <td>29.086149</td>\n",
              "      <td>138.856419</td>\n",
              "      <td>18.469595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.250000</td>\n",
              "      <td>0.138386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>11.635105</td>\n",
              "      <td>7.083572</td>\n",
              "      <td>28.079980</td>\n",
              "      <td>6.085607</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.701046</td>\n",
              "      <td>0.064732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.176471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Spam    Capitals  Punctuation  ...   Test  Words_NoPunct       Punct\n",
              "count  592.0  592.000000   592.000000  ...  592.0     592.000000  592.000000\n",
              "mean     1.0   15.320946    29.086149  ...    0.0      14.250000    0.138386\n",
              "std      0.0   11.635105     7.083572  ...    0.0       4.701046    0.064732\n",
              "min      1.0    0.000000     2.000000  ...    0.0       2.000000    0.000000\n",
              "25%      1.0    7.000000    26.000000  ...    0.0      11.000000    0.096774\n",
              "50%      1.0   14.000000    30.000000  ...    0.0      14.000000    0.137931\n",
              "75%      1.0   21.000000    34.000000  ...    0.0      18.000000    0.176471\n",
              "max      1.0  128.000000    49.000000  ...    0.0      27.000000    0.333333\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXyY5EUkIGAS",
        "outputId": "d70ca138-b64f-42b2-aa3d-8248b7b85b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# memory limit may be exceeded. Try deleting some objects before running this next section\n",
        "# or copy this section to a different notebook.\n",
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OtFF5X_IZJ4"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNrv_O2nPgD",
        "outputId": "44f96edf-13cf-4ccd-94d6-0ebdf0a8faf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "api.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'fields': {'data': '',\n",
              "    'id': 'original id inferred from folder name',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'topic': 'name of topic (20 variant of possible values)'},\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'file_size': 14483581,\n",
              "   'license': 'not found',\n",
              "   'num_records': 18846,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'parts': 3,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'fields': {'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'country': 'data from webhose.io',\n",
              "    'crawled': 'date the story was archived',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'language': 'data from webhose.io',\n",
              "    'likes': 'number of Facebook likes',\n",
              "    'main_img_url': 'image from story',\n",
              "    'ord_in_thread': '',\n",
              "    'participants_count': 'number of participants',\n",
              "    'published': 'date published',\n",
              "    'replies_count': 'number of replies',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'title': 'title of story',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'uuid': 'unique identifier'},\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'file_size': 20102776,\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'num_records': 12999,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'file_size': 3087262469,\n",
              "   'license': 'not found',\n",
              "   'num_records': 353197,\n",
              "   'parts': 2,\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'fields': {'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question'},\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'file_size': 21684784,\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'num_records': 404290,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'fields': {'RelComments': [{'RELC_DATE': 'date of posting',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RelCText': 'text of answer'}],\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'THREAD_SEQUENCE': ''},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'file_size': 234373151,\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'num_records': 189941,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'fields': {'2016-dev': ['...'],\n",
              "    '2016-test': ['...'],\n",
              "    '2016-train': ['...'],\n",
              "    '2017-test': ['...']},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'file_size': 6344358,\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'num_records': -1,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'file_name': 'text8.gz',\n",
              "   'file_size': 33182058,\n",
              "   'license': 'not found',\n",
              "   'num_records': 1701,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'record_format': 'list of str (tokens)'},\n",
              "  'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'file_size': 6516051717,\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'num_records': 4924894,\n",
              "   'parts': 4,\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'record_format': 'dict'}},\n",
              " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': []},\n",
              "  'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'file_size': 1225497562,\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'num_records': 1917247,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
              "  'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'file_size': 1005007116,\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'num_records': 999999,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
              "  'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'file_size': 405932991,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
              "  'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'file_size': 795373100,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
              "  'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'file_size': 109885004,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 25},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
              "  'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'file_size': 209216938,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
              "  'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'file_size': 134300434,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
              "  'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'file_size': 264336934,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
              "  'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'file_size': 394362229,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
              "  'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'file_size': 69182535,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
              "  'word2vec-google-news-300': {'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'file_size': 1743563840,\n",
              "   'license': 'not found',\n",
              "   'num_records': 3000000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
              "  'word2vec-ruscorpora-300': {'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'file_size': 208427381,\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'num_records': 184973,\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytG4hu4nPSB",
        "outputId": "0434b511-bc69-4f06-8486-e5cddcd6363f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lxhBMeIYrl",
        "outputId": "9c20cd10-86aa-434e-88c2-93fb4633e4e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cookie', 0.745154082775116),\n",
              " ('oatmeal_raisin_cookies', 0.6887780427932739),\n",
              " ('oatmeal_cookies', 0.662139892578125),\n",
              " ('cookie_dough_ice_cream', 0.6520504951477051),\n",
              " ('brownies', 0.6479344964027405),\n",
              " ('homemade_cookies', 0.6476464867591858),\n",
              " ('gingerbread_cookies', 0.6461867690086365),\n",
              " ('Cookies', 0.6341644525527954),\n",
              " ('cookies_cupcakes', 0.6275068521499634),\n",
              " ('cupcakes', 0.6258294582366943)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfcI1niIYQ4",
        "outputId": "7d074131-7042-4c05-8bb7-1031e54a8e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tokyo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0lCjnaImWZ",
        "outputId": "da50df0e-2366-4d4b-9d6a-82f5c7581f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.6454660892486572),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676948547363),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376776456832886),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2SSPqujo4K8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}